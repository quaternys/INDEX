{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021/06/10 -> 2023/04/29"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MeCab\n",
    "- 形態素解析\n",
    "- 頻度分析・単語 n-gram\n",
    "  - n-gram 言語モデル\n",
    "  - スムージング\n",
    "  - 実装\n",
    "- Word2Vec\n",
    "\n",
    "参考\n",
    "- 2020年度人工知能専門演習I [AI] (柴田先生)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeCab\n",
    "形態素解析ツール・ライブラリ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowsの場合\n",
    "`> pip install mecab-python3 unidic-lite`\n",
    "- たぶんこれが最速\n",
    "- ただし `-Ochasen` には非対応．\n",
    "  - 後述の辞書の問題で，ipadicなどを使えばchasenも使える．\n",
    "- [公式](https://github.com/SamuraiT/mecab-python3)によると https://learn.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist の `vc_redist.x64.exe` も必要らしい？\n",
    "- MeCab本体のインストール（推奨）\n",
    "  - https://github.com/ikegami-yukino/mecab/releases の `MeCab-64-0.996.2.exe`\n",
    "    - 文字コードはUTF-8推奨\n",
    "  - `/Program Files/MeCab/bin/` をPathに追加する（インストール先を変更していなければ）．\n",
    "\n",
    "### Colab, Linuxの場合\n",
    "```sh\n",
    "!apt install mecab-ipadic-utf8\n",
    "!ln -s /etc/mecabrc /usr/local/etc/mecabrc\n",
    "!pip install mecab-python3\n",
    "```\n",
    "\n",
    "### Macの場合\n",
    "```sh\n",
    "$ brew install mecab mecab-ipadic mecab-unidic\n",
    "$ pip install mecab-python3\n",
    "```\n",
    "- 辞書の場所：`/usr/local/lib/mecab/dic/`\n",
    "- 設定ファイル：`/usr/lobal/etc/mecabrc`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補足\n",
    "MeCabは本来Pythonライブラリでなくただの形態素解析用CLIソフト。辞書とセットで使う。\n",
    "- 環境により辞書のDLが必要。\n",
    "- 辞書により使えるコマンドも違う。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書\n",
    "- ipadic\n",
    "  - MeCab標準\n",
    "  - 2007年とかのもので古い．\n",
    "  - `pip install ipadic` し，`import ipadic; MeCab.Tagger(ipadic.MECAB_ARGS)` で使える．\n",
    "- unidic\n",
    "  - chasenには対応していない．\n",
    "- unidic-lite\n",
    "  - 軽量版unidic\n",
    "  - `pip install unidic-lite` し，そのまま使える．\n",
    "- [mecab-ipadic-NEologd](https://github.com/neologd/mecab-ipadic-neologd)\n",
    "  - 新語に強い．かつて最強とされていたが，更新は2020年で止まっている．\n",
    "  - Windowsで使う場合，辞書をコンパイルするためにWSLを経由する必要がある．\n",
    "    ```sh\n",
    "    $ sudo apt install mecab mecab-ipadic-utf8 libmecab-dev make\n",
    "    $ sudo git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "    $ cd mecab-ipadic-neologd\n",
    "    $ sudo ./bin/install-mecab-neologd -n\n",
    "    ```\n",
    "    - `Do you want to install mecab-ipadic-NEologd?` と訊かれたら `yes`．\n",
    "    - `/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/` にコンパイルされた辞書が作成される．\n",
    "    - ディレクトリごとWindowsの分かりやすいところにコピーする．\n",
    "    - 使うときは `MeCab.Tagger(\"-d コピー先/mecab-ipadic-neologd)`\n",
    "    - Windowsでも`bin/install-mecab-neologd`動く説？\n",
    "\n",
    "Python用ラッパーライブラリ\n",
    "- mecab-python3\n",
    "  - 公式\n",
    "- mecab\n",
    "  - 有志がOSによらずインストールできるようにしたもの\n",
    "- mecab-python-windows\n",
    "  - 上記の有志がwindows用に作っていたもの。現在はmecabに統合され使えない。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも も もも も もも の うち \n",
      "\n",
      "すもも\tスモモ\tすもも\t名詞-一般\t\t\n",
      "も\tモ\tも\t助詞-係助詞\t\t\n",
      "もも\tモモ\tもも\t名詞-一般\t\t\n",
      "も\tモ\tも\t助詞-係助詞\t\t\n",
      "もも\tモモ\tもも\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "うち\tウチ\tうち\t名詞-非自立-副詞可能\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wakati = MeCab.Tagger(\"-Owakati\")   # 分かち書き\n",
    "chasen = MeCab.Tagger(\"-Ochasen\")   # 分かち書き + 品詞分類\n",
    "print(wakati.parse(\"すもももももももものうち\")) # 'すもも も もも も もも の うち \\n'\n",
    "print(chasen.parse(\"すもももももももものうち\")) # 表層形、読み、原型、品詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205981, 4)\n",
      "['一' '吾輩' 'は' '猫' 'で' 'ある' '。' '名前' 'は' 'まだ']\n",
      "['名詞-数' '名詞-代名詞-一般' '助詞-係助詞' '名詞-一般' '助動詞' '助動詞' '記号-句点' '名詞-一般' '助詞-係助詞'\n",
      " '副詞-助詞類接続']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/neko.txt\", encoding=\"utf8\") as f:\n",
    "    text = f.read()\n",
    "morphemes = np.array([m[:4] for w in chasen.parse(text).split(\"\\n\") if len(m:=w.split())>=4])\n",
    "words, poses = morphemes[:, 0], morphemes[:, 3]\n",
    "print(morphemes.shape)  # 延べ語数は 205981\n",
    "print(words[:10])       # 単語\n",
    "print(poses[:10])       # 品詞"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 頻度分析・単語 n-gram\n",
    "nltk: 自然言語処理ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('の', 9194) ('。', 7486) ('て', 6873) ('、', 6772) ('は', 6422)\n",
      "('名詞-一般', 27470) ('動詞-自立', 24557) ('助詞-格助詞-一般', 20419) ('助動詞', 19810) ('助詞-接続助詞', 12067)\n",
      "('主人', 932) ('人', 355) ('迷亭', 329) ('先生', 274) ('人間', 272)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "print(*FreqDist(words).most_common(5))  # 単語頻度順\n",
    "print(*FreqDist(poses).most_common(5))  # 品詞頻度順\n",
    "print(*FreqDist(words[poses==\"名詞-一般\"]).most_common(5))  # 一般名詞頻度順"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語 n-gram\n",
    "- 「で」「ある」「。」は 3-gram (tri-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('の',), 9194), (('。',), 7486), (('て',), 6873), (('、',), 6772), (('は',), 6422)] 13589\n",
      "[(('」', '「'), 1951), (('し', 'て'), 1233), (('て', 'いる'), 1105), (('」', 'と'), 1081), (('で', 'ある'), 960)] 73151\n",
      "[(('で', 'ある', '。'), 759), (('て', 'いる', '。'), 482), (('し', 'て', 'いる'), 287), (('を', 'し', 'て'), 261), (('か', '」', '「'), 217)] 140130\n"
     ]
    }
   ],
   "source": [
    "# uni-gram, bi-gram, tri-gram の頻度分布\n",
    "for n in 1, 2, 3:\n",
    "    fd = FreqDist(ngrams(words, n))\n",
    "    print(fd.most_common(5), len(fd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram 言語モデル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **定義）** 単語列 $w_{1..n-1}$ に $w_n$ が続く確率は $$P_n(w_n | w_{1..n-1}) := \\frac{w_{1..n} の頻度}{w_{1..n-1} の頻度} $$\n",
    "- **例）**「である」に「。」が続く確率は $$ P_3(。|である) = \\frac{\"である。\" の頻度}{\"である\" の頻度} $$\n",
    "- プログラム的には $$ models[3][(\"で\", \"ある\")][\"。\"] = \\frac{fds[3][(\"で\", \"ある\", \"。\")]}{fds[2][(\"で\", \"ある\"])} = \\frac{759}{960} = 0.790625 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スムージング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Add-α**\n",
    "  - ちょっと補正（頻度に α を加算）した頻度を使う：$$ \\widehat{P}_n(w_n | w_{1..n-1}) := \\frac{w_{1..n} の頻度 + \\alpha}{w_{1..n-1} の頻度 + \\alpha K} $$\n",
    "    - $K = len(fds[1])$：異なり語数\n",
    "    - α：定数（1/Kとか）\n",
    "- Good Turing\n",
    "  - 経験カウントから、統計的に事後確率を推定する。\n",
    "- **Back-off**\n",
    "  - $P_3(。|である)=0$ の場合、代わりに $P_2(。|ある)$ をその値とする。\n",
    "    - それも 0 なら $P_1(。)$ を使う。\n",
    "- Add-α + Back-off $$ P_3(。|である) := d \\widehat{P}_2(。|ある) $$\n",
    "  - バックオフの係数 $d = \\frac{\\alpha (K - fds[2][(\"で\", \"ある\")])}{fds[2][(\"で\", \"ある\")] + \\alpha K}$\n",
    "- 補間\n",
    "  - (1-λ)(n-gramでのカウントから計算される確率) + λ((n-1)-gramでの確率)\n",
    "- Add-α + 補間 (interporation) $$ P_3(。|である) := \\widehat{P}_3(。|である) + d P_2(。|ある) $$\n",
    "  - 補間の係数 $d = \\frac{\\alpha K}{fds[2][(\"で\", \"ある\")] + \\alpha K}$\n",
    "- **Modified Kneser-Ney**\n",
    "  - 後で勉強する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    def __init__(self, n, words):\n",
    "        self.n = n\n",
    "        # fds[k] は k-gram の頻度分布。0-gram の頻度は延べ語数とする\n",
    "        fds = [{(): len(words)}] + [FreqDist(ngrams(words, k)) for k in range(1, n+1)]\n",
    "\n",
    "        # models[k][(k-1)-gramのtuple] は (k-1)-gram に続く形態素の頻度分布\n",
    "        self.models = {}\n",
    "        for k in range(1, n+1):\n",
    "            model = {prev: FreqDist() for prev in fds[k-1]}\n",
    "            for kgram, freq in fds[k].items():\n",
    "                model[kgram[:-1]][kgram[-1]] = freq / fds[k-1][kgram[:-1]]\n",
    "            self.models[k] = model\n",
    "        \n",
    "    # 文生成\n",
    "    def generate(self, start: str):\n",
    "        sentense = wakati.parse(start).split()\n",
    "        for _ in range(100):\n",
    "            for k in range(self.n, 0, -1):\n",
    "                # 未知語の次は uni-gram で最頻の「の」から続ける\n",
    "                candidates = self.models[k].get(tuple(sentense[len(sentense)-k+1:]))\n",
    "                if candidates:\n",
    "                    sentense.append(candidates.max())\n",
    "                    break\n",
    "            if sentense[-1] == \"。\":\n",
    "                break\n",
    "        return \"\".join(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtificialIntelligenceのは、いかに卒業したての理学士にせよ、あまり能がなさ過ぎる。\n",
      "メロスは激怒した。「御めえは今までに鼠を何匹とった事がある」智識は黒よりも余程発達しているつもりだが腕力と勇気とに至っては到底黒の比較にはならないと覚悟はしていたものの、この問に接したる時は、さすがに極りが善くはなかった。\n",
      "お前は今まで食ったパンの枚数を覚えているのか？」「ええ、すると会社の男が、それは死ななければ無論保険会社はいりません。\n"
     ]
    }
   ],
   "source": [
    "wv = NGram(6, words)\n",
    "print(wv.generate(\"Artificial Intelligence\"))\n",
    "print(wv.generate(\"メロスは激怒した。\"))\n",
    "print(wv.generate(\"お前は今まで食ったパンの枚数を覚えているのか？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「今日はいい天気ですね」「ジャムばかりじゃないんです、ほかに買わなけりゃ、ならない物もあります」と妻君は大に不平な気色を両頬に漲らす。\n",
      "「最低最悪だった」「まさか」と細君が小さい声を出すと、「本当ですか」と寒月君が笑う。\n",
      "「あなたの名前は何ですか」「あら御主人だって、妙なのね。\n"
     ]
    }
   ],
   "source": [
    "print(wv.generate(\"「今日はいい天気ですね」「\"))\n",
    "print(wv.generate(\"「最低最悪だった」「\"))\n",
    "print(wv.generate(\"「あなたの名前は何ですか」「\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 単語をベクトルで表現する\n",
    "- ベクトルは単語の埋め込み表現（意味みたいなもの）を考慮している\n",
    "    - 類似する単語は類似するベクトルを持つ\n",
    "    - 良いモデルでは $王 - 男 \\fallingdotseq 女王 - 女$ とかも成り立つ\n",
    "- 対義語に弱い\n",
    "    - 逆ベクトルは「無関係」な言葉になる\n",
    "    - 学習時に「学習データであるテキストにおける出現が近い単語」同士が近いベクトルになるため？\n",
    "    - https://qiita.com/youwht/items/f21325ff62603e8664e6\n",
    "- ライブラリ：gensim\n",
    "    - [公式](https://radimrehurek.com/gensim/index.html)\n",
    "    - [Gensim の Word2Vec を試す](https://qiita.com/propella/items/febc423998fd210800ca) (2021-06-22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[('猫', 0.9906332492828369), ('なる', 0.9789209961891174), ('自分', 0.9779163599014282), ('ぬ', 0.9746915698051453), ('この', 0.9734659194946289), ('する', 0.9732460379600525), ('人', 0.9717381000518799), ('彼', 0.9711594581604004), ('ため', 0.9711257219314575), ('我々', 0.9698050618171692)]\n",
      "[('人間', 0.9906331896781921), ('自分', 0.9867601990699768), ('必要', 0.9851629137992859), ('者', 0.9847874641418457), ('する', 0.9846217036247253), ('にとって', 0.9823601841926575), ('ぬ', 0.9820329546928406), ('なる', 0.9816380143165588), ('思わ', 0.9759395718574524), ('碌', 0.9759304523468018)]\n",
      "0.9906332\n",
      "[('利き', 0.8413695096969604), ('よ', 0.8046698570251465), ('」', 0.8007706999778748), ('休み', 0.7891527414321899), ('釣れ', 0.785506010055542), ('ね', 0.7767887711524963), ('です', 0.76649409532547), ('そう', 0.7653157711029053), ('生きる', 0.7622565031051636), ('もん', 0.7542473077774048)]\n"
     ]
    }
   ],
   "source": [
    "# 分かち書きから学習（分かち書きデータをファイル出力し，それをText8Corpusに投げる）\n",
    "with open(\"./data/neko_wakati.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(wakati.parse(text))\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
    "wv = Word2Vec(Text8Corpus(\"./data/neko_wakati.txt\")).wv\n",
    "print(wv[\"猫\"].shape)\n",
    "print(wv.most_similar(\"人間\"))\n",
    "print(wv.most_similar(\"猫\"))\n",
    "print(wv.similarity(\"猫\", \"人間\"))      # 類似度\n",
    "print(wv.most_similar(\"猫\", \"人間\"))    # 猫 - 人間"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデル\n",
    "- fastText\n",
    "    - 2016年にFacebookが公開したNLPライブラリ\n",
    "    - [fastTextの学習済みモデルを公開しました](https://qiita.com/Hironsan/items/513b9f93752ecee9e670)\n",
    "- word2vec-google-news-300\n",
    "    - gensim.downloader でDLできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "CPU times: total: 2min 57s\n",
      "Wall time: 8min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim.downloader as api\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[('dogs', 0.8680489659309387), ('puppy', 0.8106428384780884), ('pit_bull', 0.780396044254303), ('pooch', 0.7627376914024353), ('cat', 0.7609457969665527), ('golden_retriever', 0.7500901818275452), ('German_shepherd', 0.7465174198150635), ('Rottweiler', 0.7437615394592285), ('beagle', 0.7418621778488159), ('pup', 0.740691065788269)]\n",
      "[('cats', 0.8099379539489746), ('dog', 0.760945737361908), ('kitten', 0.7464985251426697), ('feline', 0.7326234579086304), ('beagle', 0.7150582671165466), ('puppy', 0.7075453400611877), ('pup', 0.6934291124343872), ('pet', 0.6891531348228455), ('felines', 0.6755931973457336), ('chihuahua', 0.6709762215614319)]\n",
      "0.76094574\n",
      "[('Tia_Dalma_Naomie_Harris', 0.2577422261238098), ('Mouseland', 0.2575077712535858), ('antennal', 0.25083598494529724), ('chironomid', 0.24755403399467468), ('floo', 0.2461230456829071), ('fiendish_plot', 0.2458873838186264), ('architraves', 0.2403106689453125), ('mouse', 0.23750734329223633), ('flitting', 0.23703250288963318), ('pipistrelles', 0.23694036900997162)]\n"
     ]
    }
   ],
   "source": [
    "print(wv[\"cat\"].shape)\n",
    "print(wv.most_similar(\"dog\"))\n",
    "print(wv.most_similar(\"cat\"))\n",
    "print(wv.similarity(\"cat\", \"dog\"))      # 類似度\n",
    "print(wv.most_similar(\"cat\", \"dog\"))    # 猫 - 人間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ded38025e7b08e8d11260cded3e3515995d8cb6a0dd8d6e015af437f87a8e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
